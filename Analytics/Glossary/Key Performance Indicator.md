---
tags: definition, him, kpi, metrics, rcm, analytics, data, glossary/data
author: Bryan P. Bergeron, MD
publisher: HIMSS
alias: Key Performance Indicator, KPI, KPIs, key performance indicators, key performance indicator
related:
---
# 5. Key Performance Indicators

Key Performance Indicators (KPIs) are a means, not an end. Core measures that gauge the organization’s past or current performance in particular areas, KPIs reflect how closely the organization’s underlying processes are aligned with best practices. KPI application areas range from financial administration, patient records management, personnel administration, economic analysis, facilities and equipment utilization, practice activities, and operating methods to organizational behavior, patient safety, and myriad clinical areas.

As indicators of process effectiveness and efficiency, KPIs can quantify otherwise abstract notions of quality, establish benchmarks for comparison, and serve as evidence that structure or processes have undergone positive change. Well-crafted KPIs can also facilitate accountability and provide the basis for behavior change and policy initiatives. In fulfilling these roles, KPIs
exhaustively defined variables that can provide decision makers with insight into an organization’s processes—can be as varied as the processes and outcomes they are intended to monitor.

Rarely considered alone, indicators are arranged in a logical collection referred to as a scorecard. These logical groupings can be user, department, service, or organization focused, with one scorecard feeding another. For example, Brigham and Women’s Hospital, Boston, started with
nursing service and surgical scorecards and rolled these into an executive level scorecard that contains indicators applicable across the organization. Deeper-level scorecards featuring lower-level indicators that roll up to the surgical and nursing scorecards were then developed.

Whether considered alone or arranged within a scorecard, a KPI is just that: an indicator. As an analogy, simply having the instruments used in a race car—tachometer, speedometer, and so on—will not make a commuter driving to work a more capable driver. However, in the hands of a driver focused on honing his or her skills to mimic those of a race car driver, the instruments can be invaluable.

## 5.1 KPI Taxonomy
What gets measured gets noticed, and KPIs are the focal point for that measurement. The KPI perspective can be clinical, financial, or patient focused. KPIs may also be quantitative or qualitative— or a hybrid of the two—and expressed as a fixed value, percentage, ratio, average, or rate, depending on the measure (see Table 5.1).

>Table 5.1

| **KPI Perspectives** |
| ---------------- |
| Clinical         |
| Financial        |
| Patient-focused  |
| Insurer-focused  |
| Quantitative     |
| Qualitative      |
| Hybrid           |
| Process          |
| Output           |
| Outcome          |
| Community-wide   |
| US-wide          |
| Hospital-wide    |
| Department-wide  |

Some healthcare quality organizations categorize KPIs according to their functionality. For example, the AHRQ categorizes its KPIs as process, output, or outcome. Process KPIs are used to assess whether protocols are being followed. Output KPIs are used to quantify volume and utilization. Outcome KPIs, such as mortality and infection rates, follow patient status after care. There is nothing sacred about KPI categories; they serve as a means of locating indicators that may be of use to an area within the organization.

Performance indicators can be categorized by scope and intended audience. Hospital-wide indicators are intended for review by senior management and the board. Examples include measures such as infection control, safety and security, utilization and volume, patient and employee satisfaction, and clinical risk management, including a summary of incidents and claims. Department-wide indicators are intended for review by department administrators, such as the number of vaginal deliveries for the department of OB/GYN. Blood and medication use, medical record review, and credentialing are additional examples of department-wide KPIs.

In this book, KPIs are discussed in terms of clinical and nonclinical indicators. Nonclinical indicators are used primarily to monitor parameters such as capacity and utilization, revenues, and profitability. For example, capacity and utilization KPIs are analyzed to determine whether the organization has the staff, equipment, and other resources to reach the financial goals established by the board of directors. Clinical indicators, such as postoperative infection rate and mortality, are used to assess the organization’s clinical side.

## 5.2 Selecting Indicators
There are hundreds of KPIs discussed in the following chapters and appendices, and many more are freely available online and for sale by quality organizations. Those included in this book have been compiled over nearly two decades from a variety of sources. Indicators don’t age per se, but different groups of indicators are promoted for a particular cause, by a particular organization, or from specific perspectives. For example, the National Committee for Quality Assurance (NCQA) produces a range of indicators, such as the Healthcare Effectiveness Data and Information Set (HEDIS). Similarly, the Centers for Disease Control and Prevention (CDC) (www.cdc.gov) maintains medical outcomes indicators, clinical disease indicators, and clinical and laboratory performance indicators, and the American Hospital Association (AHA) (www.aha.org) is considered one of the major standards organizations for indicator definitions in the industry. The AHA publication American Hospital Association Hospital Statistics contains definitions of industry standard indicators as well as the values for many US hospitals.

In addition to relying on nationally recognized quality groups and standards organizations, it is also common practice to develop custom KPIs to address unique problem areas of a particular healthcare organization. Regardless of whether the decision is to buy, build, or borrow, the KPIs should reflect the challenges facing the organization, the need to compare performance with
external benchmarks for accreditation, and executive management’s experience with Performance Management. At a minimum, every KPI’s name, application in decision making, formula, source of data, adjustments, and exceptions should be documented.

Although it may be tempting to simply page through a list of KPIs and select one or two “promising” indicators for each clinical department and a dozen or more for administration, doing so would be like selecting surgical instruments based on the elegance of their design and without thought to the surgical procedure. A better approach to indicator selection is to identify the high-risk, high-volume, problem-prone areas. High-risk patients, such as those admitted for acute myocardial infarctions, are typically vulnerable, fragile, and unstable. High-volume services, such as delivery in a women’s hospital, are provided to large numbers of patients. Problem-prone areas, such as lengthy emergency room (ER) wait times, vary from one hospital to the next. The downside to selecting KPIs based on risk, volume, and problem areas is that the indicators may suffer from superficiality. It can be a hit-or-miss proposition unless the problems, risks, and volumes are explicitly linked to the organization’s underlying goals and culture.

Selecting one KPI or a scorecard filled with KPIs requires foremost a shared understanding of the organization’s mission, vision, and priorities. There should also be a shared understanding of the current and future expectations of the patients who receive services from the organization.

In addition, it is critical from an IT perspective to determine whether the organization has the management expertise needed to act on the data represented by a KPI once the IT infrastructure needed to capture, translate, and properly report the indicator values has been developed.

Defining a shared vision and expectations need not involve multiple management retreats to a quiet cabin in the woods or white water rafting. Simply put: senior management and key stakeholders should be able to answer the following questions for every KPI considered:

>#### **Why is it important to the organization?**
>There should be a compelling reason why the organization should invest tens of thousands of dollars over the course of several years to monitor a particular indicator.

>#### **How does the parameter measured by the indicator affect the organization’s mission and vision?**
The relationship between the indicator and the organization’s mission and vision should be transparent and explicitly defined.

>#### **Is there an alternative, next-best indicator?**
Acquiring the data for the optimal indicator may be too expensive and take too long.

>#### **How is the indicator calculated?**
Even the simplest indicators must be precisely defined, especially when used against benchmarks or for accreditation.

>#### **What data are required, where do data reside, and how will they be acquired?**
The data for a single KPI may reside on a half-dozen different computer systems connected by a network or on a simple paper questionnaire completed by patients.

>#### **Is the current IT infrastructure adequate? If not, what additional technologies are required?**
Creating interfaces between clinical and administrative computers and building a hospital-wide data warehouse are significant IT projects. If a new IT infrastructure must be created, the actual use of KPIs could be years away.

>#### **How often will the indicator be reported?**
Some indicators only make sense when trended over years (i.e., the percentage change in indicator value from a reference or base year is calculated), while others are of value when reported daily or weekly. More frequent reporting may place unreasonable demands on a heavily loaded IT infrastructure.

>#### **How will the indicator be analyzed?**
Management and staff may need training on how to interpret process charts and other statistical reports.

>#### **What are the resource requirements in terms of time and money?**
Supporting a Performance Initiative with 40 or 50 KPIs can cost millions of dollars and require years to implement.

>#### **Can the organization control the variability measured by the indicator? If not, what is the value in knowing the indicator value?**

>#### **What will be the demands on the staff?**
If capturing the indicator is too arduous, such as requiring staff to fill out new forms for each patient encounter, staff compliance may be low.

>#### **What are the expectations of management and staff?**
Simply putting the KPI in place will not solve the underlying problem the indicator is designed to monitor. Furthermore, positive process outcomes do not necessarily translate to clinical benefits.

>#### **Is the parameter measured amenable to intervention?**
>As in clinical medicine, there is no point in wasting resources to perform a test when there is no known recourse.

At first glance, KPIs may seem deceptively simple. Take the Mortality Index, for example, which is defined as:

**Mortality = Actual Deaths / Expected Deaths**

This clinical KPI shows patient survival compared with what is expected, and a ratio of less than 1.0 is preferable. However, the Mortality Index is normally risk-adjusted for patient age, sex, and diagnosis, as well as the hospital type and size. Sicker, elderly patients have a greater likelihood of expiring during their hospital stay than, say, college athletes. Furthermore, survivability is generally better in large urban teaching hospitals than small rural hospitals.

In analyzing the indicator, exactly what constitutes a death? Does the measure include patients brought into the ER who die within five minutes? What about patients who live five hours? Five days? What about patients on mechanical ventilators and life support who are transported to other facilities? As described below, indicators should be defined in exquisite detail, including the intended use.

## 5.3 Top-Down Versus Bottom-Up
Regardless of the selection criteria, KPIs can filter throughout the organization from the department level or up from the boardroom. One of the challenges of defining indicators at the department level and then propagating them through the healthcare organization is the potential lack of congruence. What is best for a specific department may not be best for another department or the organization as a whole. For example, when multiple departments share assets, it is difficult to determine return on investment (ROI). Moreover, one department may aim to minimize use of a resource that has an expensive variable ownership cost, while another department simultaneously seeks to maximize use.

When defining performance indicators from the confines of the boardroom, the challenge lies in the potential for a poor fit at the department level. A workaround is to encourage communications and planning at the administrative level. In practice, performance indicator selection is often a hybrid approach in which top-level decisions are modified by department-level administrators with detailed knowledge of day-to-day operations.

While on the topic of indicator selection, it is important to note that indicators need not be limited to the traditional areas of finance, utilization, administration, and clinical medicine; they can serve the needs of IT as well. When Berger Clinic, part of the PeachHealth integrated delivery system in Eugene, Oregon, decided to adopt an EMR, an effective Performance Management system to gauge the system’s performance was crucial to gaining senior management support. The IT group at Berger Clinic defined KPIs related to the speed and efficiency of patient visits, reduced volumes of paper documents, reduced potential adverse drug events, and reduced costs through better utilization of clinicians and staff. The group then established a baseline measurement for each KPI and compared them with industry benchmarks and best practices. The KPIs were assessed again at 45 and 90 days after EMR go-live to assess improvement. Within 2 years of go-live, IT could demonstrate with KPI metrics the ROI provided by the EMR, in terms of financial savings and increased clinical value for patients.

## 5.4 Data Driven Versus Needs Driven
A dilemma faced by every chief information officer (CIO) involved in a Performance Management Initiative is how to balance indicator need versus data availability. One solution is to simply provide indicators based on what data are easily extracted from hospital applications. However, a better approach is to design a decision matrix in Excel in which each indicator is weighted based on organization need, technical feasibility, data availability, and cost. The matrix should be made available to the Performance Management Committee, which may choose to share it with department heads and other decision makers to counter any calls of favoritism or bias toward indicators that may benefit one department or service over another.

Although any number of decision matrices can be constructed based on local issues, a general formula that is applicable to most circumstances is:

![](https://i.imgur.com/2RwXufd.png)

*Need*, *Technical Feasibility*, *Data Availability*, and *Cost* are assigned values of 1, 2, or 3, corresponding to low, medium, or high. For example, an indicator with high cost has a *Cost* value of 3.

*Need* is an assessment of the indicator’s need, based on the Performance Management cycle defined in Chapter 3. The score assigned to *Need* may be increased because a decision maker expresses need for the indicator, because the indicator is a national benchmark that has been identified as important to the organization’s accreditation purposes, or because the indicator has been earmarked by the organization’s executive board as critical to the organization’s long-term management. *Need* should generally have the highest weighting.

Technical feasibility is an assessment, from an IT perspective, of the feasibility of providing the indicator value to decision makers. Feasibility may reflect the difficulty in extracting the data from a closed, stand-alone application or of the performance hit on a system operating near capacity that may not be able to tolerate the daily load of data extraction.

Data availability is an operational issue that relates to data’s accuracy and completeness. Data will not be available if the clinicians systematically avoid entering the ICD-9 or ICD-10 (International Classification of Diseases, 9th Revision and 10th Revision, respectively) codes in the diagnosis in the EMR, for example. Cost reflects the relative cost of acquiring, manipulating, and otherwise making the data available to decision makers. A new data warehouse, data mart, data extraction, and loading software package; ongoing system maintenance; and end user training all affect Cost.

The weights associated with each measure represent the relative importance given to each measure. For example, the weighted assigned need can be double that of the other measures (e.g., W<sub>need</sub> = 2, W<sub>tech</sub> = 1, W<sub>data</sub> = 1, and W<sub>cost</sub> = 1). In a spreadsheet, weights can be easily changed, and the relative indicator scores for the working set of indicators can be assessed.

To illustrate the utility of a decision matrix, consider that the Performance Management Committee has assembled a list of some 60 indicator candidates. Included in the list are two indicators of inpatient procedure mortality—esophageal resection mortality (ERM) and pancreatic resection mortality (PRM). The chairman of the department of surgery states that both are equally critical to assessing his department’s performance, but ERM is the higher volume indicator. For this reason, he rates ERM need as high (need = 3) and PRM need as medium (need = 2). Now, assume that providing ERM is technically challenging because the data reside in a closed, undocumented database (technical feasibility = 1) and that the surgeons have been amiss about entering the relevant ICD-9 codes in the EMR (data availability = 2). Furthermore, the cost of tracking ERM is high (cost = 3).

In comparison, providing PRM data is only modestly technically challenging (technical feasibility = 2). Data are easily available (data availability = 3), and the associated cost is modest (cost = 2). Assume further that the Performance Management Committee has assigned each measure the following weights: W<sub>need</sub> = 2, W<sub>tech</sub> = 1, W<sub>data</sub> = 1, and W<sub>cost</sub> = 2. 

The indicator scores for PRM and ERM are calculated as:

![](https://i.imgur.com/PTCE8Or.png)

According to the decision matrix calculation, PRM is ranked higher than ERM. Whether ERM appears in the final list of KPIs depends on the total number of indicators and the relative ranking of other indicators. When the matrix is defined in a spreadsheet, the Performance Management Committee members can quickly perform “what-if” scenarios by changing relative weighting.

A less technical means of deciding between needs- and data-driven KPIs is to use a phased approach that incorporates both perspectives. For example, when the Mid Western Area Health Service in New South Wales set out to establish a library of KPIs, it used a three-phased approach. Phase 1 was limited to indicators developed from data that were in the New South Wales database. Phase 2 involved refining information available in the database that was not yet in a form suitable for indicators. In Phase 3, new indicators and new data collection processes were developed.

## 5.5 Data Collection
The previous spreadsheet model of the relative merit of various measures highlights the need for high-quality, accurate data. As in any scientific experiment, the quality of the data limits the quality of the analysis. Furthermore, quality data do not appear by accident, but require a carefully designed and executed data collection plan. Such a plan should completely specify how data are observed, captured, and recorded; who is involved; where the data are collected; and how the data will be used to calculate a performance indicator.

Just as the organization’s mission drives the performance indicator, the indicator choice drives the data collection requirements. For example, in tracking medication errors, the purpose of the indicator determines whether the data collected should measure presence or absence of a drug reaction or measure the severity of a reaction. If only presence or absence is measured, then analysis will be limited to simple classification. If degree of drug reaction severity is measured, then more advanced statistical analysis is possible. Of course, collecting four or five times the data may be more labor-intensive, costly, and error-prone, compared with capturing presence or absence measures. See Chapter 11 for a more detailed discussion of data types, statistical methods, and key data collection issues, such as how to deal with missing data.

## 5.6 How Many Indicators?
What constitutes a reasonable number of indicators depends on the scope and purpose of the indicators, as summarized in Figure 5.1. 

![](https://i.imgur.com/smJtIJG.png)
>*Figure 5.1 Typical indicator count by purposes and area.*

If the goal is to demonstrate Meaningful Use, then there’s a mandatory minimum of a dozen core indicators and at least half-dozen menu set indicators. Free of external requirements, 10 to 20 indicators per department and perhaps a dozen global indicators for executive management is a reasonable long-term goal. In larger institutions, because of the complexity of each department, department heads may require more indicators than executive management. Assuming a healthcare organization has 40 major departments and a dozen indicators per department, for upper management, the total number of unique indicators that need to be defined can easily approach 200 or even 300 (there is typically overlap in indicators appropriate for each department, which decreases the total indicator count). Even a total count of only 100 indicators constitutes a major investment of time and institutional resources.

The most appropriate number and type of indicators for a given area depends on the needs and perspectives of the decision makers. As a benchmark of the most appropriate number of indicators for executive management, consider that when Mayo Clinic started on its scorecard project, it created an unwieldy KPI wish list. After assessing the needs of the organization and availability data, 14 indicators were chosen for the clinic’s initial executive-level scorecard. This number of indicators is more in line with what top management in a healthcare organization can use initially. Although seven or eight indicators may be sufficient for ranking purposes, it is not likely that the same indicators can address any unique problem areas in the organization or leave room for innovation.

The process of honing down the number of service- and department-level KPIs to a number appropriate for high-level management was also followed by Brigham and Women’s Hospital. The Boston-based hospital initially condensed about 80 indicators in its surgical and nursing scorecards into a more manageable list of 35 indicators for surgery and 30 indicators for nursing. It then created a 30-indicator executive-level scorecard composed primarily of indicators from the surgical and nursing scorecards. Similarly, when the Ontario Hospital Association (OHA) created a balanced scorecard for upper management, it identified and collected data on 38 enterprise-wide indicators.

In presenting upper management or department heads with a scorecard composed of a large number of KPIs, the danger is that the indicators may be too diverse relative to decision makers’ needs. If so, then the resulting scorecards will serve merely as distractions. Even when data are available for hundreds of indicators, it is a good idea to roll out the indicators slowly to assess the effect on decision maker behavior and to allow time for change. Another advantage of implementing a Performance Management Initiative by introducing at most two dozen KPIs at a time is the reduced burden on the training staff; this is directed at a single department or to senior management. Decision makers have to be trained on how to interpret indicator values and graphical charts and then given time to adjust their decision-making processes to accommodate the new information.

The number of KPIs actually deployed in a department or institution should not be confused with the total number of indicators that need to be developed. It is in the organization’s best interest to have a library of KPIs on hand in areas parallel to the organization’s needs. With some forethought, it is possible to plan for KPI needs and to have a pipeline of KPIs under development to service future needs. As demonstrated in Chapter 1, medication errors are a common problem in healthcare organizations. If reducing medication errors is an organizational priority, then KPIs addressing various aspects of medication ordering can be developed and included in the active list of KPIs or scorecards. Furthermore, benchmarks are frequently announced by quality organizations and healthcare organizations. If the local organization wants to compare its processes with those of the benchmark institutions, it will have to develop and deploy the relevant KPIs.

## 5.7 Defining Indicators
Indicators must be completely defined so that there is no room for misinterpretation. Most of the quality groups, including the Joint Commission on Accreditation of Healthcare Organizations (JCAHO), define what should be included in indicator definitions for accreditation. However, there is no universal indicator definition template, and technical circumstances often dictate template contents. For example, data flow diagrams should be included with complex indicators involving data from several databases. At a minimum, the definition of a KPI should include the following elements:
- #### **Name** 
>Name of KPI, using a standard naming system. The name should be self-explanatory, such as “average length of stay or ALOS.”
- #### **Internal ID Number**
>An internal ID number for tracking purposes. This number ID should be fixed and unique, for use in searching a database of KPIs.
- #### **External ID Number:**
>If applicable, an external ID number associated with a particular source, such as CMS, should be associated with the KPI. External IDs can change, based on the source. As such, it should not be the primary lookup key when storing information on the KPI.
- **Short Definition:** A one-line definition of the KPI, akin to a descriptive name, for example, “Number of days an inpatient stays in the hospital, on average, unadjusted.”
- **Long Definition:** An extensive textual definition of KPI, including sources, formulas, limitations, and applicability to the organization. The long definition should include a rationale for the organization’s use of the indicator. The rationale is especially critical in describing adjusted indicators, or indicators that make use of adjusted metrics because adjustments tend to be context specific.
- **Formula:** Mathematical equation of the KPI, less details of any adjustments. Continuing with the ALOS example: ALOS = Discharge Days / Total Discharges
- **Numerator:** Description of the numerator, including the data elements as well as populations included and excluded from the data. For example, Discharge Days often include all patients except newborns.
- **Denominator:** Description of the denominator, including the data elements as well as populations included and excluded from the data. For example, Total Discharges, like Discharge Days, often excludes newborns.
- **KPI Format:** Format of KPI output, such as days, months, or percentage. For example, ALOS = 2.3 days.
- **Data Source(s):** The sources of data used in the numerator and denominator. Sources may include a computer system, application, survey, or other data source needed to compute the KPI.
- **Collection Approach(es):** The approach(es) to collecting data for the numerator and denominator. For example, JCAHO categorizes the approaches to data collection in terms of timing, which may be retrospective, concurrent, or prospective. A retrospective approach involves the capture of events that have already occurred. A concurrent approach involves data collection on a working process, while a prospective approach involves data collected in anticipation of an event or occurrence. Following the ALOS example, Discharge Days and Total Discharges are typically collected using a retrospective approach.
- **Statistical Adjustment:** The statistical manipulations that can reduce the contribution of confounding factors, such as outliers, to the indicator value. Statistical adjustment also includes recommendations for minimizing errors introduced by the data collection process.
- **Benchmark Value(s):** External benchmark KPI values from similar organizations that are indicative of recognized best practices and internal benchmarks that provide a relative basis for process improvement.
- **Target Value:** Current target value of KPI. For example, the target value for ALOS in a given hospital may be 5.3 days. The target value can also include a qualitative improvement measure, such as an increase or decrease in the indicator value. A decrease in the length of stay is normally considered an improvement.
- **Trigger Value(s):** The out-of-bounds values, both high and low, that can be used to notify the appropriate decision makers through trigger alerts and e-mails, for example. An ALOS of less than one is and greater than six may be an appropriate trigger range, for example, depending on the patient population.
>
- **Adjustments:** The adjustments to the core KPI, including the rationale. In general, no legacy formula should be used as is for benchmark purposes.
>
- **Adjustment Formula:** Mathematical equation detailing adjustment calculation. For example, ALOS might be increased upward or downward by 10%, depending on the value of ALOS.
>
- **Recommended Analysis:** Analysis related to the KPI, such as descriptive statistics, trend analysis, horizontal analysis, and predictive modeling, for optimum interpretation of indicator data. When predictive analysis is used, a detailed definition of the curve fitting algorithm and other parameters related to the analysis must be provided. If a specific analysis software tool is used, it should be listed, with the version noted.
>
- **Department/Service Affected:** A list of the hospital departments and/or services affected by the KPI. For example, the ALOS KPI may be used by several clinical departments, medical records, and the finance department.
>
- **Process Affected:** A description of the particular processes targeted by the KPI, including a summary of any problem area(s) addressed.
>
- **KPIs Affected:** Other KPIs that use this KPI as a data element, which would be affected by changing this KPI’s definition.
>
- **Charting:** Optimal type of chart(s) used to display the KPI, for example, a boxplot or pie chart, as defined by the type of data and decisions that must be supported by the chart. The chart may be defined by the number of data elements. For example, a simple line chart may be used until 12 data points are available, at which point a process control chart may be used. The optimal charting for particular data may be specified by an accrediting institution. JCAHO’s ORYX requires the organization to evaluate its performance against both its own history, using control charts, and the other organizations’ performance, using comparison charts.
>
- **References:** A list of published articles, reports, specific URLs of quality organizations, and other sources in the following areas:
	
> **KPI:** The source of the KPI, for example, American Hospital Association, and its application.
> 
**Benchmark:** Sources of benchmark values. Reference should validate the applicability of the benchmark to the organization.
>
**Target:** Source for target value.
>
**Triggers:** A list of publications, internal benchmarks, or other sources that provide the basis for trigger values.
>
**Adjustments:** The basis for case mix, wage, and other adjustments.
>
**Related Analysis:** The basis for analysis.
>
**Charting:** The basis for chart selection. For more information, see Chapter 9 (Reporting) and the publication Tools for Performance Measurement in Healthcare: A Quick Reference Guide. 2008, Oakbrook Terrace, Illinois: Joint Commission Resources.
>
- **Security/Privacy:** A description of the security and privacy issues related to the KPI, to address local and HIPAA (Health Insurance Portability and Accountability Act) requirements. For example, a mortality indicator may be limited to distribution within the organization’s intranet, whereas other indicators may be available for public scrutiny. Financial indicators may be limited to the finance department or only high-level financial indicators may be available to department heads or physicians.
>
- **Access Level:** Related to security and privacy, the level of access to the KPI that is available to the organization’s employees and decision makers. KPIs may be viewable by physicians but not nurses, for example.
>
- **Author:** Local author(s) of the KPI.
>
- **Acronyms and Definitions:** Acronyms and definitions of processes and data elements that may not be known to readers outside of the KPI’s domain, and may vary from department to department. For example, the surgical department may use one formula for ALOS and the medicine department may use another.
>
- **Owner:** The person(s) assigned to monitor the KPI on a periodic basis; who may not be the KPI’s author or end user.
>
- **Creation Date:** Date of creation.
>
- **Frequency of Validation:** Recommended frequency of validation by the KPI owner, typically done quarterly or semiannually. Validation includes updating the latest benchmark figures, determining if the definition provided by the source organization has changed (and, if so, notifying the appropriate decision maker to assess whether a new version of the KPI definition should be constructed to reflect the change), and updating target and trigger values, if appropriate. For example:
>
Definition — Annually
Benchmark — Quarterly
Trigger — Quarterly
Target — Annually
Adjustments — Annually
References — Annually
>
- **Revision History:** A listing of dates and the nature of revisions made to the above elements.
>
- **Status:** The current status of the KPI, such as “in use,” “inactive,” or “in review.”

When reviewing the clinical and nonclinical indicator descriptions in the following chapters, readers should not consider the brief descriptions as complete definitions. Consider the short CMS/JCAHO definition of Aspirin at Arrival: “Acute myocardial infarction (AMI) patients without aspirin contraindications who received aspirin within 24 hours before or after hospital arrival.” The definition seems complete enough until the exact definition of AMI is considered. CMS/JCAHO define AMI completely by referencing a list of acceptable ICD-9 or ICD-10 codes for AMI. In addition, the complete definition identifies the excluded population as patients less than 18 years of age who transferred to another acute care hospital or federal hospital on the day of arrival; were received in transfer from another acute care hospital, including another emergency department; were discharged on the day of arrival; expired on the day of arrival; left against medical advice on the day of arrival; and had one or more aspirin contraindications/reasons for not prescribing aspirin documented in their medical record.

Even a completely defined indicator can be misused. Consider the indicators Patient Days and Patient Days (Adjusted). A Patient Day is a period of service between the census-taking hours on two successive calendar days, with the day of discharge being counted only when a patient is admitted the same day. That is, assuming a census taking hour of midnight, a patient admitted at 10 a.m. on Friday and discharged at noon on the following Saturday constitutes one Patient Day of service. As an indicator, Patient Days is useful as a utilization measure of the clinical staff’s workload and the organization’s acute fiscal status. To provide a closer approximation of these measures, Patient Days can be adjusted for inpatient revenue, contribution of outpatient services to clinical staff workload, or acuity of illness. In each case, the indicator is referred to as “Adjusted Patient Days” or “Patient Days (Adjusted).”

The most appropriate version of Patient Days depends on the purpose of the indicator. For example, unadjusted Patient Days is often used in calculating ALOS, according to the following formula:

ALOS = Patient Days/Number of Admissions

Note that this is a valid formula for ALOS, but very different from the formula for ALOS used above—hence the need to clearly indicate how this KPI is calculated. Now, in the context of comparing a hospital’s ALOS with published global benchmarks from similar institutions, unadjusted Patients Days may be a valid version of the metric. Adjusting Patient Days for revenue may be used for a comparison with other institution’s revenue-adjusted Patient Days, but should not be used to compute ALOS. Similarly, the relative contribution of outpatient services—an indicator of workload for nursing and other services—should not be used to compute ALOS. The ALOS figure will be abnormally low because outpatient volume contributes to the denominator in the formula. Acuity-adjusted Patient Days, which indicates the presence or absence of a major or minor morbidity, can be used to compute an Adjusted ALOS. Comparing Acuity-Adjusted ALOS figures enables management to make a more valid comparison of healthcare organizations serving significantly different patient demographics. Readers are cautioned not only to understand the high-level definition of an indicator but also to critically examine the underlying assumptions of applicability and to understand that the metrics are used to compute the indicator.

Precisely defined indicators provide a common language for communications on the structure and flow of business processes. Because the financial and operations processes in a healthcare organization are highly structured and standardized, the corresponding KPIs are much less open to misinterpretation compared with clinical indicators. Clinical KPIs often quantify processes that are more discretionary and free form and often never repeat in exactly the same way. For example, everyone in the boardroom understands the concept of profit margin. However, exactly what is a medication error, and what is the root cause? Patient satisfaction metrics are even more open to misinterpretation. Even so, the more tightly defined the metrics describing the indicators are, the more decision makers with different perspectives can communicate. Because intra-organization communications are critical for a successful Performance Management Initiative, the indicators selected in the nonfinancial areas should represent the more structured and repeatable elements of the clinical process, and the results should be shared and reported in a consistent manner.

## 5.8 Further Thinking
KPIs should not be defined once and then left to filter through the organization. Each indicator requires an owner who is involved from the start. Owners, who might be clinicians or administrative staff tapped by the Performance Management Committee, should be involved in the operational definition of their indicator, including its derivation, scope, and application. The owner should also take responsibility for monitoring any adjustment variables, such as shifts in patient demographics, and for monitoring external definitions of optimal indicator values that reflect new information from regulatory agencies, quality organizations, or other healthcare organizations. Finally, it is the duty of the KPI owner to periodically review the indicator for statistically significant anomalies.

The owner should not be confused with the KPI’s source or ultimate user. A KPI may be used by dozens of decision makers in the organization, none of whom are owners. Similarly, the medical records department may be the source of several KPIs, but the owners may be part of a clinical department. In addition, a single KPI may have several components that span several sources. For example, a utilization KPI, such as surgical cases per FTE, may incorporate data from human resources and the surgery department. Having a KPI owner assigned to each indicator does not obviate the need for a “super owner” who periodically examines classes of indicators. A super owner for clinical indicators would look after the clinical indicators as a whole. One of the most important factors to monitor at the indicator class level is the correlation of predictive value. Examined individually, two indicators may be up to date, accurate, and reflect the latest standards. However, taken together, the indicators may overlap in the predictive value they offer decision makers. For example, if two different measures of OR utilization consistently increase and decrease together, as though they are coupled, then monitoring both may be unnecessary. Perhaps one of the indicators could be dropped without diminishing the end user’s decision-making capabilities. The statistics behind this determination are discussed in Chapter 11.

## 5.9 Sources and Further Reading
AHA Hospital Statistics 2016. 2016. Chicago, Illinois: American Hospital Association.
CAHPS Ambulatory Care Improvement Guide: Practical Strategies for Improving Patient Experience.
Content last reviewed October 2016. Rockville, MD: Agency for Healthcare Research and Quality.
http://www.ahrq.gov/cahps/quality-improvement/improvement-guide/improvement-guide.html.
Guidance on Mode Selection in Patient Experience Surveying. 2016. Ontario Hospital Association.
Download from: www.oha.com.
hfm Magazine. Healthcare Financial Management Association. www.hfma.org.
Hospital Report Series. Canadian Institute of Health Information. https://secure.cihi.ca/estore/productSeries
.htm?pc=PCC219.
Journal for Healthcare Quality. National Association of Healthcare Quality. www.nahq.org/education /journal
-healthcare-quality.
Journal of Healthcare Management. American College of Healthcare Executives. www.ache.org/Publications
/SubscriptionPurchase.aspx#jhm.
Kohn, L., J. Corrigan, and M. Donaldson, eds. To Err Is Human: Building a Safer Health System. 2000,
National Academy Press: Washington, DC.
Kottke, T.E. and G.J. Isham. Measuring health care access and quality to improve health in populations.
Prev Chronic Dis 2010, 7(4): A73. www.cdc.gov/pcd/issues/2010/jul/09_0243.htm. Accessed April 13,
2017.
National Forum for Health Care Quality Measurement and Reporting (NQF). www.qualityforum.org.
Paton, J. Clinical performance indicators–good, bad, or ugly duckling? Br J Healthcare Comput Info Manage
2002, 19(5): 26–8.
Performance Measurement for Hospitals. Joint Commission. http://www.jointcomission.org. Accessed April 26,
2017.
Rodak, S. 16 Potential Key Performance Indicators for Hospitals. 2013. Becker’s Hospital Review. www
.beckershospitalreview.com/strategic-planning/16-potential-key-performance-indicators-for-hospitals .html.
Accessed April 25, 2017.
Schilp, J. and R. Gilbreath. Health Data Quest: How to Find and Use Data for Performance Improvement.
2000, San Francisco: Jossey-Bass.
Starr, G., T. Rogers, M. Schooley, S. Porter, E. Wiesen, and N. Jamison. Key Outcome Indicators for
Evaluating Comprehensive Tobacco Control Programs. 2005. Atlanta, GA: Centers for Disease Control
and Prevention.
Vancil, R. Uncommon denominators: Metrics that matter. CMO Magazine 2004, 1(3): 54–5.